"""
AI æ‰¹æ¬¡è™•ç†æ¨¡çµ„
================
ä½¿ç”¨ Claude API å°å·²æ“·å–çš„æ–‡ç« é€²è¡Œåˆ†é¡ã€æ‘˜è¦ã€é—œéµè³‡è¨Šæå–ã€‚

ç”¨æ³•ï¼ˆCLIï¼‰ï¼š
  # æƒæä¸¦åˆ—å‡ºæ–‡ç« 
  python ai_processor.py --scan ~/vet-articles

  # è™•ç†æ‰€æœ‰æœªè™•ç†çš„æ–‡ç« 
  python ai_processor.py --process ~/vet-articles

  # å¼·åˆ¶é‡æ–°è™•ç†æ‰€æœ‰æ–‡ç« 
  python ai_processor.py --process ~/vet-articles --force
"""

import json
import os
import re
import sys
import logging
import time
import argparse
from datetime import datetime
from pathlib import Path
from typing import Optional

import yaml

logger = logging.getLogger(__name__)

# ============================================================
# ç¸é†«å°ˆæ¥­åˆ†é¡é«”ç³»
# ============================================================

VET_CATEGORIES = {
    "å…§ç§‘": ["è…è‡Ÿ", "å¿ƒè‡Ÿ", "å…§åˆ†æ³Œ", "è…«ç˜¤", "æ„ŸæŸ“ç—‡", "æ¶ˆåŒ–", "å‘¼å¸", "ç¥ç¶“"],
    "å¤–ç§‘": ["éª¨ç§‘", "è»Ÿçµ„ç¹”", "çœ¼ç§‘", "ç‰™ç§‘"],
    "æ€¥è¨ºèˆ‡é‡ç—‡": [],
    "å½±åƒè¨ºæ–·": [],
    "è‡¨åºŠç—…ç†": [],
    "ç‡Ÿé¤Šå­¸": [],
    "è¡Œç‚ºå­¸": [],
    "å…¬å…±è¡›ç”Ÿ": [],
    "è—¥ç†å­¸": [],
    "å…¶ä»–": [],
}

# æ‰€æœ‰å­é¡åˆ¥çš„å±•å¹³åˆ—è¡¨ï¼ˆç”¨æ–¼ promptï¼‰
ALL_SUBCATEGORIES = []
for cat, subs in VET_CATEGORIES.items():
    if subs:
        for sub in subs:
            ALL_SUBCATEGORIES.append(f"{cat}/{sub}")
    else:
        ALL_SUBCATEGORIES.append(cat)

# ============================================================
# è¨­å®š
# ============================================================

DEFAULT_MODEL = "claude-sonnet-4-20250514"
DEFAULT_MAX_TOKENS = 2000
DEFAULT_API_DELAY = 1.0  # æ¯æ¬¡ API å‘¼å«ä¹‹é–“çš„é–“éš”ï¼ˆç§’ï¼‰
MAX_ARTICLE_CHARS = 8000  # è¶…éæ­¤é•·åº¦çš„æ–‡ç« æœƒè¢«æˆªæ–·

# API é‡è©¦è¨­å®š
MAX_API_RETRIES = 3
API_RETRY_BASE_DELAY = 2.0  # åŸºç¤é‡è©¦å»¶é²ï¼ˆç§’ï¼‰ï¼ŒæŒ‡æ•¸é€€é¿
API_RATE_LIMIT_DELAY = 30.0  # 429 rate limit æ™‚çš„ç­‰å¾…æ™‚é–“ï¼ˆç§’ï¼‰

# ============================================================
# Anthropic SDKï¼ˆé¸ç”¨ï¼‰
# ============================================================

try:
    import anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False


# ============================================================
# Frontmatter è§£æå’Œæ›´æ–°
# ============================================================

_FRONTMATTER_RE = re.compile(r'^---\s*\n(.*?)\n---\s*\n', re.DOTALL)


def parse_frontmatter(content: str) -> tuple[dict, str]:
    """è§£æ YAML frontmatterï¼ˆä½¿ç”¨ PyYAMLï¼‰ã€‚

    Args:
        content: å®Œæ•´çš„ Markdown å…§å®¹

    Returns:
        (frontmatter_dict, body_content) â€” frontmatter å­—å…¸å’Œæ­£æ–‡
    """
    match = _FRONTMATTER_RE.match(content)
    if not match:
        return {}, content

    fm_block = match.group(1)
    body = content[match.end():]

    try:
        fm = yaml.safe_load(fm_block)
        if not isinstance(fm, dict):
            fm = {}
    except yaml.YAMLError:
        logger.warning(f"YAML frontmatter è§£æå¤±æ•—ï¼Œå›å‚³ç©ºå­—å…¸")
        fm = {}

    # æ­£è¦åŒ–ï¼šæŠŠ None å€¼è½‰ç‚ºç©ºå­—ä¸²ï¼ˆä¿æŒå‘å¾Œç›¸å®¹ï¼‰
    for key, value in fm.items():
        if value is None:
            fm[key] = ""

    return fm, body


def update_frontmatter(content: str, updates: dict) -> str:
    """æ›´æ–° frontmatter æ¬„ä½ï¼Œä¿ç•™å…¶ä»–æ¬„ä½ä¸è®Šï¼ˆä½¿ç”¨ PyYAMLï¼‰ã€‚

    Args:
        content: å®Œæ•´çš„ Markdown å…§å®¹
        updates: è¦æ›´æ–°/æ–°å¢çš„æ¬„ä½

    Returns:
        æ›´æ–°å¾Œçš„å®Œæ•´ Markdown å…§å®¹
    """
    fm, body = parse_frontmatter(content)
    fm.update(updates)

    fm_str = yaml.dump(
        fm,
        allow_unicode=True,
        default_flow_style=False,
        sort_keys=False,
    )

    return f"---\n{fm_str}---\n{body}"


# ============================================================
# æ–‡ç« æƒæ
# ============================================================

def scan_articles(output_dir: str) -> list[dict]:
    """æƒæè¼¸å‡ºç›®éŒ„ä¸­çš„æ–‡ç« ã€‚

    Args:
        output_dir: æ–‡ç« è¼¸å‡ºç›®éŒ„è·¯å¾‘

    Returns:
        æ–‡ç« åˆ—è¡¨ï¼Œæ¯ç¯‡åŒ…å« path, title, platform, has_ai_data, char_count
    """
    output_dir = os.path.expanduser(output_dir)
    articles = []

    if not os.path.isdir(output_dir):
        logger.warning(f"ç›®éŒ„ä¸å­˜åœ¨ï¼š{output_dir}")
        return articles

    for entry in sorted(os.listdir(output_dir)):
        entry_path = os.path.join(output_dir, entry)
        if not os.path.isdir(entry_path):
            continue

        content_path = os.path.join(entry_path, "content.md")
        if not os.path.isfile(content_path):
            continue

        try:
            with open(content_path, "r", encoding="utf-8") as f:
                content = f.read()
        except (IOError, UnicodeDecodeError) as e:
            logger.warning(f"ç„¡æ³•è®€å– {content_path}ï¼š{e}")
            continue

        fm, body = parse_frontmatter(content)

        # åˆ¤æ–·æ˜¯å¦å·²æœ‰ AI è™•ç†è³‡æ–™
        has_ai_data = bool(fm.get("category")) and bool(fm.get("summary"))

        # è®€å– metadata.json ä»¥ç²å–æ›´å¤šè³‡è¨Š
        meta_path = os.path.join(entry_path, "metadata.json")
        platform = fm.get("platform", "")
        if os.path.isfile(meta_path):
            try:
                with open(meta_path, "r", encoding="utf-8") as f:
                    meta = json.load(f)
                platform = platform or meta.get("platform", "")
            except (json.JSONDecodeError, IOError):
                pass

        articles.append({
            "path": entry_path,
            "title": fm.get("title", entry),
            "platform": platform,
            "has_ai_data": has_ai_data,
            "char_count": len(body),
        })

    return articles


def estimate_cost(articles: list[dict], model: str = DEFAULT_MODEL) -> dict:
    """ä¼°ç®— AI è™•ç†è²»ç”¨ã€‚

    Args:
        articles: æ–‡ç« åˆ—è¡¨ï¼ˆä¾†è‡ª scan_articlesï¼‰
        model: ä½¿ç”¨çš„æ¨¡å‹åç¨±

    Returns:
        åŒ…å« article_count, total_chars, estimated_input_tokens,
        estimated_output_tokens, estimated_cost_usd, model çš„å­—å…¸
    """
    unprocessed = [a for a in articles if not a.get("has_ai_data")]
    total_chars = sum(min(a["char_count"], MAX_ARTICLE_CHARS) for a in unprocessed)

    # ä¸­æ–‡å­—å…ƒ token å¯†åº¦è¼ƒé«˜ï¼ˆç´„ 1.5-2 token/charï¼‰ï¼ŒåŠ ä¸Š system prompt
    system_prompt_tokens = 500  # ç³»çµ±æç¤ºå¤§ç´„ 500 token
    estimated_input_tokens = int(total_chars / 2.5) + system_prompt_tokens * len(unprocessed)
    estimated_output_tokens = 300 * len(unprocessed)  # æ¯ç¯‡å›å‚³ç´„ 300 token

    # å®šåƒ¹ï¼ˆä»¥ Sonnet ç‚ºåŸºæº–ï¼‰
    # Input: $3/M tokens, Output: $15/M tokens
    if "sonnet" in model.lower():
        input_cost_per_m = 3.0
        output_cost_per_m = 15.0
    elif "haiku" in model.lower():
        input_cost_per_m = 0.25
        output_cost_per_m = 1.25
    elif "opus" in model.lower():
        input_cost_per_m = 15.0
        output_cost_per_m = 75.0
    else:
        input_cost_per_m = 3.0
        output_cost_per_m = 15.0

    cost = (estimated_input_tokens * input_cost_per_m +
            estimated_output_tokens * output_cost_per_m) / 1_000_000

    return {
        "article_count": len(unprocessed),
        "total_articles": len(articles),
        "total_chars": total_chars,
        "estimated_input_tokens": estimated_input_tokens,
        "estimated_output_tokens": estimated_output_tokens,
        "estimated_cost_usd": round(cost, 4),
        "model": model,
    }


# ============================================================
# Claude API äº’å‹•
# ============================================================

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½è³‡æ·±ç¸é†«å¸«å’Œé†«å­¸æ–‡ç»åˆ†æå°ˆå®¶ã€‚è«‹åˆ†æä»¥ä¸‹ç¸é†«ç›¸é—œæ–‡ç« ï¼Œä¸¦æä¾›çµæ§‹åŒ–çš„åˆ†é¡å’Œæ‘˜è¦ã€‚

## åˆ†é¡é«”ç³»

è«‹å¾ä»¥ä¸‹é¡åˆ¥ä¸­é¸æ“‡æœ€åˆé©çš„ï¼š

- å…§ç§‘ï¼šè…è‡Ÿ / å¿ƒè‡Ÿ / å…§åˆ†æ³Œ / è…«ç˜¤ / æ„ŸæŸ“ç—‡ / æ¶ˆåŒ– / å‘¼å¸ / ç¥ç¶“
- å¤–ç§‘ï¼šéª¨ç§‘ / è»Ÿçµ„ç¹” / çœ¼ç§‘ / ç‰™ç§‘
- æ€¥è¨ºèˆ‡é‡ç—‡
- å½±åƒè¨ºæ–·
- è‡¨åºŠç—…ç†
- ç‡Ÿé¤Šå­¸
- è¡Œç‚ºå­¸
- å…¬å…±è¡›ç”Ÿ
- è—¥ç†å­¸
- å…¶ä»–

## å›å‚³æ ¼å¼

è«‹ä»¥ JSON æ ¼å¼å›å‚³ï¼Œçµæ§‹å¦‚ä¸‹ï¼š
```json
{
  "category": "ä¸»é¡åˆ¥",
  "subcategory": "å­é¡åˆ¥ï¼ˆå¦‚æœæœ‰çš„è©±ï¼Œå¦å‰‡ç‚ºç©ºå­—ä¸²ï¼‰",
  "tags": ["æ¨™ç±¤1", "æ¨™ç±¤2", "æ¨™ç±¤3"],
  "summary": "50-150 å­—çš„ä¸€æ®µè©±æ‘˜è¦",
  "key_points": ["è‡¨åºŠé‡é»1", "è‡¨åºŠé‡é»2", "è‡¨åºŠé‡é»3"],
  "clinical_relevance": "ä¸€å¥è©±èªªæ˜å°è‡¨åºŠå¯¦å‹™çš„æ„ç¾©"
}
```

## æ³¨æ„äº‹é …

- æ¨™ç±¤è«‹ä½¿ç”¨ä¸­æ–‡ï¼Œ3-8 å€‹
- æ‘˜è¦æ§åˆ¶åœ¨ 50-150 å­—
- é—œéµè¦é» 3-5 å€‹
- å¦‚æœæ–‡ç« éç¸é†«ç›¸é—œï¼Œcategory è¨­ç‚º "å…¶ä»–"
- åªå›å‚³ JSONï¼Œä¸è¦åŠ å…¶ä»–èªªæ˜æ–‡å­—"""


def _build_user_prompt(article_text: str, title: str = "") -> str:
    """å»ºæ§‹ä½¿ç”¨è€…ç«¯çš„ promptã€‚"""
    header = f"æ–‡ç« æ¨™é¡Œï¼š{title}\n\n" if title else ""
    # æˆªæ–·éé•·çš„æ–‡ç« 
    if len(article_text) > MAX_ARTICLE_CHARS:
        article_text = article_text[:MAX_ARTICLE_CHARS] + "\n\n[... æ–‡ç« å·²æˆªæ–· ...]"
    return f"{header}ä»¥ä¸‹æ˜¯æ–‡ç« å…§å®¹ï¼š\n\n{article_text}"


def _is_retryable_api_error(error) -> bool:
    """åˆ¤æ–· API éŒ¯èª¤æ˜¯å¦å€¼å¾—é‡è©¦ã€‚

    å¯é‡è©¦ï¼š429 RateLimitError, 5xx InternalServerError,
            APIConnectionError, APITimeoutError
    ä¸å¯é‡è©¦ï¼š401 AuthenticationError, 400 BadRequestError,
              å…¶ä»– 4xx éŒ¯èª¤
    """
    if not HAS_ANTHROPIC:
        return False

    # ç¶²è·¯é€£ç·šå’Œè¶…æ™‚ â†’ é‡è©¦
    if isinstance(error, (anthropic.APIConnectionError, anthropic.APITimeoutError)):
        return True
    # Rate limit â†’ é‡è©¦ï¼ˆä½†å»¶é²æ›´é•·ï¼‰
    if isinstance(error, anthropic.RateLimitError):
        return True
    # 5xx server error â†’ é‡è©¦
    if isinstance(error, anthropic.InternalServerError):
        return True
    # 401, 400, å…¶ä»– 4xx â†’ ä¸é‡è©¦
    if isinstance(error, (anthropic.AuthenticationError, anthropic.BadRequestError)):
        return False
    # å…¶ä»– APIStatusError â†’ æª¢æŸ¥ status code
    if isinstance(error, anthropic.APIStatusError):
        return error.status_code >= 500
    return False


def process_single_article(
    article_text: str,
    api_key: str,
    model: str = DEFAULT_MODEL,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    title: str = "",
) -> dict:
    """ç”¨ Claude API è™•ç†å–®ç¯‡æ–‡ç« ï¼ˆå«è‡ªå‹•é‡è©¦ï¼‰ã€‚

    Args:
        article_text: æ–‡ç« æ­£æ–‡
        api_key: Anthropic API Key
        model: æ¨¡å‹åç¨±
        max_tokens: æœ€å¤§å›å‚³ token æ•¸
        title: æ–‡ç« æ¨™é¡Œï¼ˆå¯é¸ï¼Œå¢åŠ ä¸Šä¸‹æ–‡ï¼‰

    Returns:
        åŒ…å« category, subcategory, tags, summary, key_points, clinical_relevance çš„å­—å…¸

    Raises:
        ImportError: æœªå®‰è£ anthropic
        RuntimeError: API å‘¼å«æˆ–å›æ‡‰è§£æå¤±æ•—ï¼ˆå·²é‡è©¦ä»å¤±æ•—ï¼‰
    """
    if not HAS_ANTHROPIC:
        raise ImportError(
            "anthropic å¥—ä»¶æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ pip install anthropic"
        )

    client = anthropic.Anthropic(api_key=api_key)
    user_prompt = _build_user_prompt(article_text, title)

    # API å‘¼å«ï¼ˆå«æŒ‡æ•¸é€€é¿é‡è©¦ï¼‰
    message = None
    last_error = None

    for attempt in range(MAX_API_RETRIES):
        try:
            message = client.messages.create(
                model=model,
                max_tokens=max_tokens,
                system=SYSTEM_PROMPT,
                messages=[{"role": "user", "content": user_prompt}],
            )
            break  # æˆåŠŸï¼Œè·³å‡ºé‡è©¦è¿´åœˆ

        except anthropic.APIError as e:
            last_error = e

            if not _is_retryable_api_error(e):
                # ä¸å¯é‡è©¦çš„éŒ¯èª¤ï¼ˆ401, 400 ç­‰ï¼‰ï¼Œç›´æ¥å¤±æ•—
                raise RuntimeError(f"Claude API éŒ¯èª¤ï¼ˆä¸å¯é‡è©¦ï¼‰ï¼š{e}") from e

            if attempt < MAX_API_RETRIES - 1:
                # è¨ˆç®—é‡è©¦å»¶é²
                if isinstance(e, anthropic.RateLimitError):
                    delay = API_RATE_LIMIT_DELAY
                    logger.warning(
                        f"[AI] ğŸš« é­é‡é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {delay}s å¾Œé‡è©¦ "
                        f"({attempt + 1}/{MAX_API_RETRIES})"
                    )
                else:
                    delay = API_RETRY_BASE_DELAY * (2 ** attempt)
                    logger.warning(
                        f"[AI] âš ï¸ API éŒ¯èª¤ï¼š{e}ï¼Œ{delay}s å¾Œé‡è©¦ "
                        f"({attempt + 1}/{MAX_API_RETRIES})"
                    )
                time.sleep(delay)
            # å¦‚æœæ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œè¿´åœˆçµæŸå¾Œæœƒè™•ç†

    if message is None:
        raise RuntimeError(
            f"Claude API å‘¼å«åœ¨é‡è©¦ {MAX_API_RETRIES} æ¬¡å¾Œä»ç„¶å¤±æ•—ï¼š{last_error}"
        ) from last_error

    # è§£æå›æ‡‰
    response_text = message.content[0].text.strip()

    # å˜—è©¦ç›´æ¥è§£æ JSON
    try:
        result = json.loads(response_text)
    except json.JSONDecodeError:
        # å˜—è©¦å¾ markdown code block ä¸­æå– JSON
        json_match = re.search(r'```(?:json)?\s*\n(.*?)\n```', response_text, re.DOTALL)
        if json_match:
            try:
                result = json.loads(json_match.group(1))
            except json.JSONDecodeError:
                raise RuntimeError(
                    f"ç„¡æ³•è§£æ Claude å›æ‡‰ç‚º JSONï¼š{response_text[:200]}"
                )
        else:
            raise RuntimeError(
                f"ç„¡æ³•è§£æ Claude å›æ‡‰ç‚º JSONï¼š{response_text[:200]}"
            )

    # é©—è­‰å¿…è¦æ¬„ä½
    required_keys = {"category", "tags", "summary"}
    missing = required_keys - set(result.keys())
    if missing:
        raise RuntimeError(f"Claude å›æ‡‰ç¼ºå°‘æ¬„ä½ï¼š{missing}")

    # æ­£è¦åŒ–
    result.setdefault("subcategory", "")
    result.setdefault("key_points", [])
    result.setdefault("clinical_relevance", "")

    # ç¢ºä¿ tags æ˜¯åˆ—è¡¨
    if isinstance(result["tags"], str):
        result["tags"] = [t.strip() for t in result["tags"].split(",")]

    return result


def process_article_batch(
    articles: list[dict],
    api_key: str,
    model: str = DEFAULT_MODEL,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    api_delay: float = DEFAULT_API_DELAY,
    on_progress: Optional[callable] = None,
    cancel_event=None,
) -> dict:
    """æ‰¹æ¬¡è™•ç†å¤šç¯‡æ–‡ç« ã€‚

    Args:
        articles: æ–‡ç« åˆ—è¡¨ï¼ˆä¾†è‡ª scan_articlesï¼Œæ¯ç¯‡éœ€å« pathï¼‰
        api_key: Anthropic API Key
        model: æ¨¡å‹åç¨±
        max_tokens: æœ€å¤§å›å‚³ token æ•¸
        api_delay: API å‘¼å«é–“éš”ï¼ˆç§’ï¼‰
        on_progress: é€²åº¦å›èª¿ (current, total, message)
        cancel_event: threading.Eventï¼Œè¨­å®šæ™‚å–æ¶ˆè™•ç†

    Returns:
        {"success": int, "failed": int, "results": list[dict]}
    """
    total = len(articles)
    success_count = 0
    failed_count = 0
    results = []

    for i, article in enumerate(articles, 1):
        if cancel_event and cancel_event.is_set():
            logger.info("AI è™•ç†å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")
            break

        title = article.get("title", "æœªçŸ¥")
        path = article.get("path", "")

        if on_progress:
            on_progress(i, total, f"æ­£åœ¨è™•ç†ï¼š{title}")

        try:
            # è®€å–æ–‡ç« å…§å®¹
            content_path = os.path.join(path, "content.md")
            with open(content_path, "r", encoding="utf-8") as f:
                content = f.read()

            fm, body = parse_frontmatter(content)

            # å‘¼å« Claude API
            ai_result = process_single_article(
                body, api_key, model, max_tokens, title=title,
            )

            # æ›´æ–° frontmatter
            fm_updates = {
                "category": (f"{ai_result['category']}/{ai_result['subcategory']}"
                             if ai_result.get("subcategory")
                             else ai_result["category"]),
                "tags": ai_result.get("tags", []),
                "summary": ai_result.get("summary", ""),
                "key_points": ai_result.get("key_points", []),
                "clinical_relevance": ai_result.get("clinical_relevance", ""),
            }
            updated_content = update_frontmatter(content, fm_updates)

            with open(content_path, "w", encoding="utf-8") as f:
                f.write(updated_content)

            # æ›´æ–° metadata.json
            meta_path = os.path.join(path, "metadata.json")
            if os.path.isfile(meta_path):
                try:
                    with open(meta_path, "r", encoding="utf-8") as f:
                        meta = json.load(f)
                except (json.JSONDecodeError, IOError):
                    meta = {}
            else:
                meta = {}

            meta.update({
                "category": fm_updates["category"],
                "tags": ai_result.get("tags", []),
                "summary": ai_result.get("summary", ""),
                "ai_model": model,
                "ai_processed_at": datetime.now().isoformat(),
            })

            with open(meta_path, "w", encoding="utf-8") as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)
                f.write("\n")

            results.append({
                "title": title,
                "status": "success",
                "category": fm_updates["category"],
                "path": path,
            })
            success_count += 1
            logger.info(f"[AI] âœ… {title} â†’ {fm_updates['category']}")

        except Exception as e:
            logger.error(f"[AI] âŒ {title}ï¼š{e}")
            results.append({
                "title": title,
                "status": "failed",
                "error": str(e),
                "path": path,
            })
            failed_count += 1

        # API å‘¼å«é–“éš”
        if i < total and not (cancel_event and cancel_event.is_set()):
            time.sleep(api_delay)

    if on_progress:
        on_progress(total, total, "AI è™•ç†å®Œæˆ")

    return {
        "success": success_count,
        "failed": failed_count,
        "results": results,
    }


# ============================================================
# CLI å…¥å£
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="AI æ–‡ç« æ‰¹æ¬¡è™•ç†å·¥å…·")
    parser.add_argument("--scan", metavar="DIR", help="æƒææ–‡ç« ç›®éŒ„")
    parser.add_argument("--process", metavar="DIR", help="è™•ç†æ–‡ç« ç›®éŒ„")
    parser.add_argument("--force", action="store_true", help="å¼·åˆ¶é‡æ–°è™•ç†æ‰€æœ‰æ–‡ç« ")
    parser.add_argument("--model", default=DEFAULT_MODEL, help="Claude æ¨¡å‹åç¨±")
    parser.add_argument("--delay", type=float, default=DEFAULT_API_DELAY,
                        help="API å‘¼å«é–“éš”ï¼ˆç§’ï¼‰")
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO, format="%(message)s")

    if args.scan:
        articles = scan_articles(args.scan)
        print(f"\næ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ï¼š")
        for a in articles:
            status = "âœ… å·²è™•ç†" if a["has_ai_data"] else "â¬œ æœªè™•ç†"
            print(f"  {status} | {a['platform']:12s} | {a['title']}")

        cost = estimate_cost(articles, args.model)
        print(f"\næœªè™•ç†ï¼š{cost['article_count']} ç¯‡")
        print(f"é ä¼°è²»ç”¨ï¼š~${cost['estimated_cost_usd']:.4f} USD ({cost['model']})")

    elif args.process:
        api_key = os.environ.get("ANTHROPIC_API_KEY", "")
        if not api_key:
            print("éŒ¯èª¤ï¼šè«‹è¨­å®š ANTHROPIC_API_KEY ç’°å¢ƒè®Šæ•¸")
            sys.exit(1)

        articles = scan_articles(args.process)
        if not args.force:
            articles = [a for a in articles if not a["has_ai_data"]]

        if not articles:
            print("æ²’æœ‰éœ€è¦è™•ç†çš„æ–‡ç« ")
            return

        print(f"å³å°‡è™•ç† {len(articles)} ç¯‡æ–‡ç« ...")
        cost = estimate_cost(
            [{"char_count": a["char_count"], "has_ai_data": False} for a in articles],
            args.model,
        )
        print(f"é ä¼°è²»ç”¨ï¼š~${cost['estimated_cost_usd']:.4f} USD")

        def progress_cb(current, total, msg):
            print(f"  [{current}/{total}] {msg}")

        result = process_article_batch(
            articles, api_key, model=args.model,
            api_delay=args.delay, on_progress=progress_cb,
        )
        print(f"\nå®Œæˆï¼æˆåŠŸï¼š{result['success']}ï¼Œå¤±æ•—ï¼š{result['failed']}")

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
