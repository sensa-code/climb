#!/usr/bin/env python3
"""
ç¸é†«æ–‡ç« è‡ªå‹•åŒ–æ“·å–å·¥å…·
===========================
ä¸‰å±¤ç­–ç•¥ï¼šJina Readerï¼ˆé¦–é¸ï¼‰â†’ BeautifulSoupï¼ˆå‚™é¸ï¼‰â†’ Playwrightï¼ˆå…œåº•ï¼‰
æ”¯æ´å¹³å°ï¼šPTTã€Mediumã€æ–°èç¶²ç«™ã€éƒ¨è½æ ¼ã€ç¸é†«å­¸æœƒç¶²ç«™ç­‰

ç”¨æ³•ï¼š
  # å–®ä¸€ URL
  python scraper.py https://example.com/article

  # æ‰¹æ¬¡ï¼ˆå¾æª”æ¡ˆè®€å– URL åˆ—è¡¨ï¼‰
  python scraper.py --batch urls.txt

  # æŒ‡å®šè¼¸å‡ºç›®éŒ„
  python scraper.py https://example.com/article --output ./my_articles
"""

import os
import re
import sys
import json
import time
import hashlib
import argparse
import logging
import urllib.robotparser
import urllib.request
import urllib.error
from datetime import datetime
from functools import lru_cache
from pathlib import Path
from urllib.parse import urlparse, urljoin

import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md

# ============================================================
# è¨­å®š
# ============================================================

# é è¨­å€¼ï¼ˆå¯è¢« config.json è¦†è“‹ï¼‰
_DEFAULTS = {
    "output_dir": os.path.expanduser("~/vet-articles"),
    "request_timeout": 30,
    "max_retries": 3,
    "retry_base_delay": 2,
    "politeness_delay": 2,
    "jina_base_url": "https://r.jina.ai/",
    "log_level": "INFO",
}


def load_config(config_path: str = None) -> dict:
    """è¼‰å…¥è¨­å®šæª”ï¼Œæœªæ‰¾åˆ°å‰‡ç”¨é è¨­å€¼"""
    config = dict(_DEFAULTS)
    if config_path is None:
        config_path = Path(__file__).parent / "config.json"
    else:
        config_path = Path(config_path)

    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
            config.update(user_config)
            # å±•é–‹ ~ è·¯å¾‘
            if "output_dir" in user_config:
                config["output_dir"] = os.path.expanduser(config["output_dir"])
        except (json.JSONDecodeError, TypeError):
            pass  # è¨­å®šæª”æå£æ™‚ä½¿ç”¨é è¨­å€¼

    return config


# è¼‰å…¥è¨­å®šï¼ˆæ¨¡çµ„å±¤ç´šï¼Œä¾›å„å‡½å¼ä½¿ç”¨ï¼‰
_CONFIG = load_config()

DEFAULT_OUTPUT_DIR = _CONFIG["output_dir"]
REQUEST_TIMEOUT = _CONFIG["request_timeout"]
JINA_BASE_URL = _CONFIG["jina_base_url"]
JINA_API_KEY = os.environ.get("JINA_API_KEY", "")  # è¨­å®šå¾Œå¯æå‡è‡³ 200 æ¬¡/åˆ†é˜
IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg', '.bmp'}
MAX_RETRIES = _CONFIG["max_retries"]
RETRY_BASE_DELAY = _CONFIG["retry_base_delay"]
POLITENESS_DELAY = _CONFIG["politeness_delay"]
DEDUP_FILE = ".fetched_urls.json"  # å·²ä¸‹è¼‰ URL è¨˜éŒ„æª”

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
}

logger = logging.getLogger(__name__)


def _setup_logging(log_dir: str = None, level: str = "INFO"):
    """è¨­å®š console + file é›™è¼¸å‡ºæ—¥èªŒ"""
    logger.setLevel(logging.DEBUG)

    # é¿å…é‡è¤‡æ·»åŠ  handler
    if logger.handlers:
        return logger

    # Console handlerï¼ˆä¿æŒåŸæœ‰è¡Œç‚ºï¼‰
    console = logging.StreamHandler()
    console.setLevel(getattr(logging, level.upper(), logging.INFO))
    console.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', '%H:%M:%S'))
    logger.addHandler(console)

    # File handlerï¼ˆDEBUG ç­‰ç´šï¼ŒæŒ‰æ—¥æœŸåˆ†æª”ï¼‰
    if log_dir:
        log_path = Path(log_dir) / "logs"
        log_path.mkdir(parents=True, exist_ok=True)
        log_file = log_path / f"scraper_{datetime.now().strftime('%Y%m%d')}.log"

        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s [%(levelname)s] %(name)s - %(message)s'
        ))
        logger.addHandler(file_handler)

    return logger


# ============================================================
# robots.txt æª¢æŸ¥
# ============================================================

@lru_cache(maxsize=128)
def _get_robots_parser(domain: str):
    """å–å¾—ä¸¦å¿«å–æŒ‡å®š domain çš„ robots.txt è§£æå™¨"""
    robots_url = f"{domain}/robots.txt"
    parser = urllib.robotparser.RobotFileParser()
    parser.set_url(robots_url)
    try:
        resp = urllib.request.urlopen(robots_url, timeout=5)
        content = resp.read().decode("utf-8", errors="surrogateescape")
        parser.parse(content.splitlines())
        return parser
    except Exception:
        return None


def is_allowed_by_robots(url: str, user_agent: str = "*") -> bool:
    """æª¢æŸ¥ robots.txt æ˜¯å¦å…è¨±æ“·å–æ­¤ URLï¼ˆfail-openï¼šç„¡æ³•å–å¾—æ™‚å…è¨±ï¼‰"""
    try:
        parsed = urlparse(url)
        domain = f"{parsed.scheme}://{parsed.netloc}"
        parser = _get_robots_parser(domain)
        if parser is None:
            return True
        return parser.can_fetch(user_agent, url)
    except Exception:
        return True


# ============================================================
# é‡è©¦æ©Ÿåˆ¶ï¼ˆæŒ‡æ•¸é€€é¿ï¼‰
# ============================================================

def retry_fetch(func, url: str, max_retries: int = MAX_RETRIES) -> dict | None:
    """å¸¶æŒ‡æ•¸é€€é¿çš„é‡è©¦åŒ…è£å™¨"""
    for attempt in range(max_retries):
        result = func(url)
        if result is not None:
            return result
        if attempt < max_retries - 1:
            delay = RETRY_BASE_DELAY * (2 ** attempt)
            logger.info(f"  ç¬¬ {attempt + 1} æ¬¡å¤±æ•—ï¼Œ{delay} ç§’å¾Œé‡è©¦...")
            time.sleep(delay)
    return None


# ============================================================
# å»é‡æ©Ÿåˆ¶
# ============================================================

def _load_dedup_record(output_dir: str) -> set:
    """è¼‰å…¥å·²ä¸‹è¼‰çš„ URL è¨˜éŒ„"""
    dedup_path = Path(output_dir) / DEDUP_FILE
    if dedup_path.exists():
        try:
            data = json.loads(dedup_path.read_text(encoding='utf-8'))
            return set(data)
        except (json.JSONDecodeError, TypeError):
            return set()
    return set()


def _save_dedup_record(output_dir: str, fetched_urls: set):
    """å„²å­˜å·²ä¸‹è¼‰çš„ URL è¨˜éŒ„"""
    dedup_path = Path(output_dir) / DEDUP_FILE
    dedup_path.parent.mkdir(parents=True, exist_ok=True)
    dedup_path.write_text(
        json.dumps(sorted(fetched_urls), ensure_ascii=False, indent=2),
        encoding='utf-8'
    )


def is_already_fetched(url: str, output_dir: str) -> bool:
    """æª¢æŸ¥ URL æ˜¯å¦å·²ç¶“ä¸‹è¼‰é"""
    fetched = _load_dedup_record(output_dir)
    return url in fetched


def mark_as_fetched(url: str, output_dir: str):
    """å°‡ URL æ¨™è¨˜ç‚ºå·²ä¸‹è¼‰"""
    fetched = _load_dedup_record(output_dir)
    fetched.add(url)
    _save_dedup_record(output_dir, fetched)


# ============================================================
# YAML å®‰å…¨è½‰ç¾©
# ============================================================

def _yaml_safe_title(title: str) -> str:
    """ç¢ºä¿æ¨™é¡Œå¯å®‰å…¨æ”¾å…¥ YAML frontmatter"""
    title = title.replace('\\', '\\\\').replace('"', '\\"')
    return title


# ============================================================
# ç¬¬ä¸€æ­¥ï¼šå¹³å°è­˜åˆ¥
# ============================================================

PLATFORM_RULES = [
    # (åç¨±, åŸŸåé—œéµå­—åˆ—è¡¨, æ˜¯å¦éœ€è¦ç™»å…¥, å»ºè­°ç­–ç•¥)
    ("PTT",        ["ptt.cc"],                          False, "jina"),
    ("Medium",     ["medium.com"],                      False, "jina"),
    ("ç—å®¢é‚¦",     ["pixnet.net"],                       False, "jina"),
    ("æ–¹æ ¼å­",     ["vocus.cc"],                         False, "jina"),
    ("LINE TODAY", ["today.line.me"],                    False, "jina"),
    ("ç¸é†«å­¸æœƒ",   ["vetmed.org.tw", "tava.org.tw",
                    "avat.org.tw"],                      False, "bs4"),
    ("æ–°èç¶²ç«™",   ["udn.com", "ltn.com.tw",
                    "ettoday.net", "setn.com",
                    "chinatimes.com", "tvbs.com.tw",
                    "cna.com.tw"],                       False, "jina"),
    ("éƒ¨è½æ ¼",     ["blogspot.com", "wordpress.com",
                    "blog."],                            False, "jina"),
    # é›£çˆ¬å¹³å°ï¼ˆæé†’ç”¨æˆ¶ä½¿ç”¨ Chrome Extensionï¼‰
    ("Facebook",   ["facebook.com", "fb.com",
                    "fb.watch"],                         True,  "skip"),
    ("Instagram",  ["instagram.com"],                   True,  "skip"),
    ("å¾®ä¿¡å…¬çœ¾è™Ÿ", ["mp.weixin.qq.com"],                 True,  "playwright"),
    ("å°ç´…æ›¸",     ["xiaohongshu.com", "xhslink.com"],  True,  "playwright"),
]


def identify_platform(url: str) -> dict:
    """è­˜åˆ¥ URL æ‰€å±¬å¹³å°ï¼Œå›å‚³å¹³å°è³‡è¨Š"""
    parsed = urlparse(url)
    domain = parsed.netloc.lower()

    for name, keywords, needs_login, strategy in PLATFORM_RULES:
        for kw in keywords:
            if kw in domain:
                return {
                    "name": name,
                    "domain": domain,
                    "needs_login": needs_login,
                    "strategy": strategy,
                }

    # æœªçŸ¥å¹³å°ï¼Œé è¨­ç”¨ Jina
    return {
        "name": "å…¶ä»–",
        "domain": domain,
        "needs_login": False,
        "strategy": "jina",
    }


# ============================================================
# ç¬¬äºŒæ­¥ï¼šJina Reader ç­–ç•¥ï¼ˆé¦–é¸ï¼Œå…è²»ï¼‰
# ============================================================

def fetch_with_jina(url: str) -> dict | None:
    """ç”¨ Jina Reader æ“·å–ç¶²é ï¼Œå›å‚³ Markdown å…§å®¹"""
    jina_url = f"{JINA_BASE_URL}{url}"
    headers = {
        **HEADERS,
        'Accept': 'text/markdown',
    }
    if JINA_API_KEY:
        headers['Authorization'] = f'Bearer {JINA_API_KEY}'

    try:
        logger.info(f"[Jina] æ­£åœ¨æ“·å–ï¼š{url}")
        resp = requests.get(jina_url, headers=headers, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()

        content = resp.text.strip()

        # é©—è­‰å…§å®¹æœ‰æ•ˆæ€§
        if len(content) < 100:
            logger.warning("[Jina] å…§å®¹å¤ªçŸ­ï¼Œå¯èƒ½æ“·å–å¤±æ•—")
            return None

        # å˜—è©¦å¾å…§å®¹æå–æ¨™é¡Œï¼ˆJina é€šå¸¸ç¬¬ä¸€è¡Œæ˜¯ Title:ï¼‰
        title = _extract_title_from_jina(content)

        return {
            "title": title,
            "content": content,
            "source": "jina",
            "url": url,
        }

    except requests.exceptions.RequestException as e:
        logger.warning(f"[Jina] æ“·å–å¤±æ•—ï¼š{e}")
        return None


def _extract_title_from_jina(content: str) -> str:
    """å¾ Jina å›å‚³çš„å…§å®¹æå–æ¨™é¡Œ"""
    lines = content.split('\n')
    for line in lines[:5]:
        line = line.strip()
        # Jina æ ¼å¼ï¼šTitle: xxxxx
        if line.startswith('Title:'):
            return line[6:].strip()
        # Markdown H1
        if line.startswith('# '):
            return line[2:].strip()
    return "æœªå‘½åæ–‡ç« "


# ============================================================
# ç¬¬ä¸‰æ­¥ï¼šHTML è§£æï¼ˆBS4 å’Œ Playwright å…±ç”¨ï¼‰
# ============================================================

def _parse_ptt_article(soup, url: str, source: str = "bs4") -> dict | None:
    """PTT å°ˆç”¨è§£æå™¨ â€” è™•ç† #main-content çµæ§‹"""
    main = soup.find('div', id='main-content')
    if not main:
        return None

    # æå– meta è³‡è¨Šï¼ˆä½œè€…ã€çœ‹æ¿ã€æ¨™é¡Œã€æ™‚é–“ï¼‰
    meta = {}
    for metaline in main.select('div.article-metaline'):
        tag = metaline.select_one('span.article-meta-tag')
        value = metaline.select_one('span.article-meta-value')
        if tag and value:
            meta[tag.get_text(strip=True)] = value.get_text(strip=True)

    title = meta.get('æ¨™é¡Œ', '')

    # ç§»é™¤ metaline å’Œæ¨æ–‡ï¼Œåªç•™æ­£æ–‡
    for tag in main.select('div.article-metaline, div.article-metaline-right, '
                           'div.push, span.f2'):
        tag.decompose()

    # å–å¾—æ­£æ–‡ï¼ˆPTT æ­£æ–‡æ˜¯ #main-content å…§çš„æ–‡å­—ç¯€é»ï¼‰
    content = main.get_text(separator='\n').strip()

    # æ¸…ç†ï¼šç§»é™¤å‰å¾Œå¤šé¤˜çš„ç©ºè¡Œ
    lines = content.splitlines()
    # ç§»é™¤é–‹é ­çš„ç©ºè¡Œ
    while lines and not lines[0].strip():
        lines.pop(0)
    # ç§»é™¤çµå°¾çš„ "--" åˆ†éš”ç·šåŠä¹‹å¾Œçš„å…§å®¹ï¼ˆé€šå¸¸æ˜¯ç°½åæª”ï¼‰
    for i, line in enumerate(lines):
        if line.strip() == '--':
            lines = lines[:i]
            break
    content = '\n'.join(lines).strip()

    # æå–åœ–ç‰‡
    images = []
    for img in main.find_all('img'):
        src = img.get('src') or img.get('data-src')
        if src:
            images.append(urljoin(url, src))

    # ä¹Ÿå¾æ–‡å­—ä¸­æ‰¾åœ–ç‰‡é€£çµï¼ˆPTT å¸¸ç”¨ imgur ç­‰ç›´é€£ï¼‰
    for line in content.splitlines():
        line = line.strip()
        if re.match(r'https?://\S+\.(jpg|jpeg|png|gif|webp)', line, re.I):
            if line not in images:
                images.append(line)

    if not content or len(content) < 30:
        return None

    return {
        "title": title or "æœªå‘½åæ–‡ç« ",
        "content": content,
        "source": source,
        "url": url,
        "images": images,
        "meta": meta,
    }


def _parse_html_to_article(html: str, url: str, source: str = "bs4") -> dict | None:
    """å°‡ HTML è§£æç‚º article dictï¼ˆBS4 å’Œ Playwright å…±ç”¨ï¼‰"""
    soup = BeautifulSoup(html, 'html.parser')

    # PTT å°ˆç”¨è§£æ
    parsed = urlparse(url)
    if 'ptt.cc' in parsed.netloc:
        result = _parse_ptt_article(soup, url, source)
        if result:
            return result
        # å¦‚æœ PTT å°ˆç”¨è§£æå¤±æ•—ï¼Œç¹¼çºŒç”¨é€šç”¨é‚è¼¯

    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
    for tag in soup.find_all(['script', 'style', 'nav', 'footer',
                               'header', 'aside', 'iframe', 'noscript']):
        tag.decompose()

    # å˜—è©¦æ‰¾åˆ°ä¸»è¦å…§å®¹å€åŸŸ
    article = (
        soup.find('article') or
        soup.find('div', class_=re.compile(r'article|content|post|entry', re.I)) or
        soup.find('div', id=re.compile(r'article|content|post|entry', re.I)) or
        soup.find('main') or
        soup.body
    )

    if not article:
        logger.warning(f"[{source}] æ‰¾ä¸åˆ°ä¸»è¦å…§å®¹")
        return None

    # è½‰æ›æˆ Markdown
    content = md(str(article), heading_style="ATX", strip=['img'])

    # æå–åœ–ç‰‡
    images = []
    for img in article.find_all('img'):
        src = img.get('src') or img.get('data-src') or img.get('data-original')
        if src:
            full_url = urljoin(url, src)
            images.append(full_url)

    # é‡å»ºå«åœ–ç‰‡çš„ Markdown
    for i, img_url in enumerate(images):
        content += f"\n\n![åœ–ç‰‡{i+1}]({img_url})"

    # æå–æ¨™é¡Œ
    title = ""
    title_tag = soup.find('title')
    if title_tag:
        title = title_tag.get_text(strip=True)
    h1 = soup.find('h1')
    if h1:
        title = h1.get_text(strip=True)

    if len(content.strip()) < 50:
        logger.warning(f"[{source}] å…§å®¹å¤ªçŸ­")
        return None

    return {
        "title": title or "æœªå‘½åæ–‡ç« ",
        "content": content.strip(),
        "source": source,
        "url": url,
    }


def fetch_with_bs4(url: str) -> dict | None:
    """ç”¨ requests + BeautifulSoup æ“·å–ç¶²é """
    try:
        logger.info(f"[BS4] æ­£åœ¨æ“·å–ï¼š{url}")
        # PTT éœ€è¦ over18 cookie æ‰èƒ½å­˜å–å…§å®¹
        cookies = {}
        parsed = urlparse(url)
        if 'ptt.cc' in parsed.netloc:
            cookies['over18'] = '1'
        resp = requests.get(url, headers=HEADERS, cookies=cookies,
                            timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()
        resp.encoding = resp.apparent_encoding or 'utf-8'
        return _parse_html_to_article(resp.text, url, source="bs4")

    except requests.exceptions.RequestException as e:
        logger.warning(f"[BS4] æ“·å–å¤±æ•—ï¼š{e}")
        return None


# ============================================================
# ç¬¬ä¸‰æ­¥ä¹‹äºŒï¼šPlaywright ç­–ç•¥ï¼ˆå…œåº•ï¼Œè™•ç† JS æ¸²æŸ“é é¢ï¼‰
# ============================================================

def fetch_with_playwright(url: str) -> dict | None:
    """ç”¨ Playwright ç„¡é ­ç€è¦½å™¨æ“·å– JS æ¸²æŸ“é é¢"""
    try:
        from playwright.sync_api import sync_playwright
    except ImportError:
        logger.warning("[Playwright] æœªå®‰è£ playwrightï¼Œè·³éæ­¤ç­–ç•¥")
        return None

    try:
        logger.info(f"[Playwright] æ­£åœ¨æ“·å–ï¼š{url}")
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent=HEADERS['User-Agent'],
                locale='zh-TW'
            )

            # PTT éœ€è¦ over18 cookie
            parsed = urlparse(url)
            if 'ptt.cc' in parsed.netloc:
                context.add_cookies([{
                    'name': 'over18',
                    'value': '1',
                    'domain': '.ptt.cc',
                    'path': '/',
                }])

            page = context.new_page()
            page.goto(url, timeout=30000, wait_until='networkidle')
            html = page.content()
            browser.close()

        return _parse_html_to_article(html, url, source="playwright")

    except Exception as e:
        logger.warning(f"[Playwright] æ“·å–å¤±æ•—ï¼š{e}")
        return None


# ============================================================
# ç¬¬å››æ­¥ï¼šè‡ªå‹•é™ç´šæ“·å–
# ============================================================

def fetch_article(url: str) -> dict | None:
    """
    è‡ªå‹•è­˜åˆ¥å¹³å°ä¸¦ç”¨æœ€ä½³ç­–ç•¥æ“·å–æ–‡ç« ã€‚
    å« robots.txt æª¢æŸ¥ã€é‡è©¦æ©Ÿåˆ¶ã€é™ç´šé †åºï¼šJina â†’ BS4 â†’ Playwright
    """
    platform = identify_platform(url)
    logger.info(f"å¹³å°è­˜åˆ¥ï¼š{platform['name']} ({platform['domain']})")

    # éœ€è¦ç™»å…¥çš„å¹³å°ï¼Œæé†’ç”¨æˆ¶
    if platform["strategy"] == "skip":
        logger.warning(
            f"âš ï¸  {platform['name']} éœ€è¦ç™»å…¥æ‰èƒ½æ“·å–ï¼Œ"
            f"å»ºè­°ä½¿ç”¨ Chrome Extension æ‰‹å‹•å„²å­˜ï¼"
        )
        return None

    # robots.txt æª¢æŸ¥
    if not is_allowed_by_robots(url):
        logger.warning(f"ğŸš« robots.txt ä¸å…è¨±æ“·å–ï¼š{url}")
        return None

    # æ ¹æ“šå»ºè­°ç­–ç•¥æ±ºå®šå˜—è©¦é †åº
    if platform["strategy"] == "playwright":
        strategies = [("playwright", fetch_with_playwright), ("bs4", fetch_with_bs4)]
    elif platform["strategy"] == "bs4":
        strategies = [("bs4", fetch_with_bs4), ("jina", fetch_with_jina)]
    else:
        strategies = [("jina", fetch_with_jina), ("bs4", fetch_with_bs4)]

    # Playwright ä½œç‚ºæ‰€æœ‰ç­–ç•¥çš„æœ€çµ‚å…œåº•
    if ("playwright", fetch_with_playwright) not in strategies:
        strategies.append(("playwright", fetch_with_playwright))

    for name, func in strategies:
        result = retry_fetch(func, url)
        if result:
            logger.info(f"âœ… æˆåŠŸæ“·å–ï¼ˆç­–ç•¥ï¼š{name}ï¼‰")
            result["platform"] = platform["name"]
            return result
        logger.info(f"[{name}] æ‰€æœ‰é‡è©¦å‡å¤±æ•—ï¼Œå˜—è©¦ä¸‹ä¸€å€‹ç­–ç•¥...")

    logger.error(f"âŒ æ‰€æœ‰ç­–ç•¥éƒ½å¤±æ•—ï¼š{url}")
    return None


# ============================================================
# ç¬¬äº”æ­¥ï¼šåœ–ç‰‡ä¸‹è¼‰èˆ‡å…§å®¹ä¿å­˜
# ============================================================

def download_image(img_url: str, save_path: Path, referer: str = "") -> bool:
    """ä¸‹è¼‰å–®å¼µåœ–ç‰‡"""
    try:
        headers = {**HEADERS}
        if referer:
            headers['Referer'] = referer

        resp = requests.get(img_url, headers=headers, timeout=15, stream=True)
        resp.raise_for_status()

        with open(save_path, 'wb') as f:
            for chunk in resp.iter_content(8192):
                f.write(chunk)
        return True

    except Exception as e:
        logger.warning(f"åœ–ç‰‡ä¸‹è¼‰å¤±æ•—ï¼š{img_url} â€” {e}")
        return False


def save_article(article: dict, output_dir: str = DEFAULT_OUTPUT_DIR) -> Path:
    """
    å„²å­˜æ–‡ç« ç‚º Markdownï¼Œä¸‹è¼‰åœ–ç‰‡åˆ°æœ¬åœ°ã€‚
    ç›®éŒ„çµæ§‹ï¼šoutput_dir/YYYY-MM-DD_æ¨™é¡Œ/content.md
    """
    title = article["title"]
    # æ¸…ç†æ¨™é¡Œä¸­çš„ç‰¹æ®Šå­—å…ƒ
    safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)[:60]
    date_str = datetime.now().strftime("%Y-%m-%d")
    folder_name = f"{date_str}_{safe_title}"

    # é˜²æ­¢æ¨™é¡Œç¢°æ’ï¼šè³‡æ–™å¤¾å·²å­˜åœ¨æ™‚åŠ ä¸ŠçŸ­ hash
    article_dir = Path(output_dir) / folder_name
    if article_dir.exists():
        url_hash = hashlib.md5(article.get("url", "").encode()).hexdigest()[:6]
        folder_name = f"{folder_name}_{url_hash}"
        article_dir = Path(output_dir) / folder_name
    images_dir = article_dir / "images"
    article_dir.mkdir(parents=True, exist_ok=True)
    images_dir.mkdir(exist_ok=True)

    content = article["content"]

    # æå–ä¸¦ä¸‹è¼‰åœ–ç‰‡
    img_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
    img_matches = re.findall(img_pattern, content)

    # ä¹Ÿæ‰¾ç´” URL åœ–ç‰‡
    url_pattern = r'(https?://[^\s<>"\']+\.(?:jpg|jpeg|png|gif|webp)(?:\?[^\s<>"\']*)?)'
    url_matches = re.findall(url_pattern, content)

    all_images = []
    for alt, url in img_matches:
        all_images.append(url)
    for url in url_matches:
        if url not in all_images:
            all_images.append(url)

    # æ±ºå®š Referer
    referer = article.get("url", "")
    parsed = urlparse(referer)
    referer_base = f"{parsed.scheme}://{parsed.netloc}/"

    # ä¸‹è¼‰åœ–ç‰‡ä¸¦æ›¿æ›è·¯å¾‘
    for i, img_url in enumerate(all_images, 1):
        ext = _guess_extension(img_url)
        local_name = f"img_{i:02d}{ext}"
        local_path = images_dir / local_name

        if download_image(img_url, local_path, referer=referer_base):
            # æ›¿æ›å…§å®¹ä¸­çš„åœ–ç‰‡è·¯å¾‘
            content = content.replace(img_url, f"images/{local_name}")
            logger.info(f"  ğŸ“· åœ–ç‰‡ {i}: {local_name}")

    # çµ„è£æœ€çµ‚ Markdownï¼ˆYAML å®‰å…¨è½‰ç¾©æ¨™é¡Œï¼‰
    yaml_title = _yaml_safe_title(title)
    metadata = f"""---
title: "{yaml_title}"
source: {article.get('url', 'N/A')}
platform: {article.get('platform', 'unknown')}
fetched_by: {article.get('source', 'unknown')}
date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
---

"""
    final_content = metadata + content

    # å„²å­˜
    md_path = article_dir / "content.md"
    md_path.write_text(final_content, encoding='utf-8')

    # åŒæ™‚å„²å­˜åŸå§‹ JSONï¼ˆæ–¹ä¾¿å¾ŒçºŒæ‰¹æ¬¡è™•ç†ï¼‰
    meta_path = article_dir / "metadata.json"
    meta_path.write_text(json.dumps({
        "title": title,
        "url": article.get("url"),
        "platform": article.get("platform"),
        "source": article.get("source"),
        "fetched_at": datetime.now().isoformat(),
        "image_count": len(all_images),
    }, ensure_ascii=False, indent=2), encoding='utf-8')

    logger.info(f"ğŸ’¾ å·²å„²å­˜ï¼š{md_path}")
    return article_dir


def _guess_extension(url: str) -> str:
    """å¾ URL çŒœæ¸¬åœ–ç‰‡å‰¯æª”å"""
    parsed = urlparse(url)
    path = parsed.path.lower()
    for ext in IMAGE_EXTENSIONS:
        if path.endswith(ext):
            return ext
    return '.jpg'  # é è¨­


# ============================================================
# ç¬¬å…­æ­¥ï¼šæ‰¹æ¬¡è™•ç†
# ============================================================

def batch_fetch_urls(urls: list, output_dir: str = DEFAULT_OUTPUT_DIR) -> dict:
    """
    è™•ç† URL åˆ—è¡¨çš„æ‰¹æ¬¡æ“·å–ã€‚
    ä¾› batch_fetch()ã€fetch_ptt_board() ç­‰å…±ç”¨ã€‚
    """
    logger.info(f"å…± {len(urls)} å€‹ URL å¾…æ“·å–")

    results = {"success": [], "failed": [], "skipped": []}

    for i, url in enumerate(urls, 1):
        logger.info(f"\n--- [{i}/{len(urls)}] ---")

        # å»é‡æª¢æŸ¥
        if is_already_fetched(url, output_dir):
            logger.info(f"å·²ä¸‹è¼‰éï¼Œè·³éï¼š{url}")
            results["skipped"].append({"url": url, "reason": "å·²ä¸‹è¼‰é"})
            continue

        platform = identify_platform(url)
        if platform["strategy"] == "skip":
            logger.warning(f"è·³é {platform['name']} å¹³å°ï¼š{url}")
            results["skipped"].append({"url": url, "reason": f"{platform['name']} éœ€è¦ç™»å…¥"})
            continue

        article = fetch_article(url)
        if article:
            save_path = save_article(article, output_dir)
            mark_as_fetched(url, output_dir)
            results["success"].append({"url": url, "path": str(save_path)})
        else:
            results["failed"].append({"url": url})

        # ç¦®è²Œå»¶é²ï¼Œé¿å…è¢«å°
        if i < len(urls):
            time.sleep(POLITENESS_DELAY)

    # è¼¸å‡ºçµ±è¨ˆ
    logger.info(f"\n{'='*50}")
    logger.info(f"æ“·å–å®Œæˆï¼")
    logger.info(f"   æˆåŠŸï¼š{len(results['success'])}")
    logger.info(f"   å¤±æ•—ï¼š{len(results['failed'])}")
    logger.info(f"   è·³éï¼š{len(results['skipped'])}")

    # å„²å­˜å ±å‘Š
    report_path = Path(output_dir) / f"batch_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    report_path.parent.mkdir(parents=True, exist_ok=True)
    report_path.write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding='utf-8')
    logger.info(f"   å ±å‘Šï¼š{report_path}")

    return results


def batch_fetch(url_file: str, output_dir: str = DEFAULT_OUTPUT_DIR) -> dict:
    """
    å¾æª”æ¡ˆè®€å– URL åˆ—è¡¨ï¼Œæ‰¹æ¬¡æ“·å–ã€‚
    URL æª”æ¡ˆæ ¼å¼ï¼šæ¯è¡Œä¸€å€‹ URLï¼Œ# é–‹é ­ç‚ºè¨»è§£
    """
    urls = []
    with open(url_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                urls.append(line)

    return batch_fetch_urls(urls, output_dir)


# ============================================================
# ç¬¬ä¸ƒæ­¥ï¼šPTT çœ‹æ¿åˆ—è¡¨é çˆ¬å–
# ============================================================

def fetch_ptt_board(board: str, pages: int = 1, output_dir: str = DEFAULT_OUTPUT_DIR) -> list:
    """
    å¾ PTT çœ‹æ¿åˆ—è¡¨é æå–æ–‡ç«  URLï¼Œæ”¯æ´ç¿»é ã€‚

    Args:
        board: çœ‹æ¿åç¨±ï¼ˆå¦‚ 'cat', 'dog', 'Vet'ï¼‰
        pages: è¦çˆ¬å–çš„é æ•¸ï¼ˆå¾æœ€æ–°é å¾€å›ï¼‰
        output_dir: è¼¸å‡ºç›®éŒ„ï¼ˆç”¨æ–¼å»é‡æª¢æŸ¥ï¼‰
    Returns:
        å»é‡å¾Œçš„æ–‡ç«  URL åˆ—è¡¨
    """
    base_url = f"https://www.ptt.cc/bbs/{board}/index.html"
    cookies = {'over18': '1'}
    collected_urls = []
    current_url = base_url

    for page_num in range(pages):
        logger.info(f"[PTT] æ­£åœ¨è®€å–çœ‹æ¿ {board} ç¬¬ {page_num + 1}/{pages} é ...")
        try:
            resp = requests.get(current_url, headers=HEADERS, cookies=cookies,
                                timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
        except requests.exceptions.RequestException as e:
            logger.warning(f"[PTT] çœ‹æ¿é é¢è®€å–å¤±æ•—ï¼š{e}")
            break

        soup = BeautifulSoup(resp.text, 'html.parser')

        # æå–æ–‡ç« é€£çµ
        for entry in soup.select('div.r-ent'):
            link = entry.select_one('div.title a')
            if link and link.get('href'):
                full_url = urljoin('https://www.ptt.cc', link['href'])
                if not is_already_fetched(full_url, output_dir):
                    collected_urls.append(full_url)

        # æ‰¾ä¸Šä¸€é é€£çµ
        prev_link = None
        for btn in soup.select('div.btn-group-paging a.btn.wide'):
            if 'ä¸Šé ' in btn.text:
                prev_link = urljoin('https://www.ptt.cc', btn['href'])
                break

        if not prev_link or page_num >= pages - 1:
            break
        current_url = prev_link
        time.sleep(1)  # ç¦®è²Œå»¶é²

    logger.info(f"[PTT] å¾ {board} çœ‹æ¿å–å¾— {len(collected_urls)} ç¯‡æ–°æ–‡ç«  URL")
    return collected_urls


# ============================================================
# ç¬¬å…«æ­¥ï¼šæ’ç¨‹è‡ªå‹•åŸ·è¡Œ
# ============================================================

def run_scheduled(args):
    """æ’ç¨‹æ¨¡å¼ï¼šå®šæœŸè‡ªå‹•åŸ·è¡Œçˆ¬èŸ²ä»»å‹™"""
    try:
        import schedule
    except ImportError:
        logger.error("æ’ç¨‹æ¨¡å¼éœ€è¦ schedule å¥—ä»¶ï¼špip install schedule")
        sys.exit(1)

    def job():
        logger.info("[æ’ç¨‹] é–‹å§‹åŸ·è¡Œå®šæ™‚ä»»å‹™...")
        if args.ptt_board:
            urls = fetch_ptt_board(args.ptt_board, args.pages, args.output)
            if urls:
                batch_fetch_urls(urls, args.output)
            else:
                logger.info("[æ’ç¨‹] æ²’æœ‰æ–°æ–‡ç« ")
        elif args.batch:
            batch_fetch(args.batch, args.output)
        else:
            logger.warning("[æ’ç¨‹] æ’ç¨‹æ¨¡å¼éœ€æ­é… --ptt-board æˆ– --batch ä½¿ç”¨")
            return
        logger.info("[æ’ç¨‹] ä»»å‹™å®Œæˆï¼Œç­‰å¾…ä¸‹æ¬¡åŸ·è¡Œ...")

    interval = args.schedule
    schedule.every(interval).minutes.do(job)

    logger.info(f"[æ’ç¨‹] å·²å•Ÿå‹•ï¼Œæ¯ {interval} åˆ†é˜åŸ·è¡Œä¸€æ¬¡ï¼ˆCtrl+C åœæ­¢ï¼‰")
    job()  # ç«‹å³åŸ·è¡Œç¬¬ä¸€æ¬¡

    try:
        while True:
            schedule.run_pending()
            time.sleep(10)
    except KeyboardInterrupt:
        logger.info("[æ’ç¨‹] å·²åœæ­¢")


# ============================================================
# CLI å…¥å£
# ============================================================

def main():
    global _CONFIG, DEFAULT_OUTPUT_DIR, REQUEST_TIMEOUT, MAX_RETRIES
    global RETRY_BASE_DELAY, POLITENESS_DELAY, JINA_BASE_URL

    parser = argparse.ArgumentParser(
        description="ğŸ¾ ç¸é†«æ–‡ç« è‡ªå‹•åŒ–æ“·å–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ï¼š
  # æ“·å–å–®ç¯‡æ–‡ç« 
  python scraper.py https://www.ptt.cc/bbs/dog/M.xxxxx.html

  # æ‰¹æ¬¡æ“·å–ï¼ˆå¾æª”æ¡ˆè®€å– URLï¼‰
  python scraper.py --batch urls.txt

  # æŒ‡å®šè¼¸å‡ºç›®éŒ„
  python scraper.py https://example.com --output ~/obsidian/vet-articles

  # åªè­˜åˆ¥å¹³å°ï¼ˆä¸æ“·å–ï¼‰
  python scraper.py https://facebook.com/some-post --identify

  # PTT çœ‹æ¿è‡ªå‹•çˆ¬å–ï¼ˆæœ€æ–° 3 é ï¼‰
  python scraper.py --ptt-board cat --pages 3

  # æ’ç¨‹æ¨¡å¼ï¼ˆæ¯ 60 åˆ†é˜è‡ªå‹•åŸ·è¡Œï¼‰
  python scraper.py --ptt-board cat --pages 2 --schedule 60
        """
    )
    parser.add_argument("url", nargs="?", help="è¦æ“·å–çš„ç¶²é  URL")
    parser.add_argument("--batch", "-b", help="æ‰¹æ¬¡æ¨¡å¼ï¼šURL åˆ—è¡¨æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--output", "-o", default=DEFAULT_OUTPUT_DIR,
                        help=f"è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­ï¼š{DEFAULT_OUTPUT_DIR}ï¼‰")
    parser.add_argument("--identify", "-i", action="store_true",
                        help="åªè­˜åˆ¥å¹³å°ï¼Œä¸æ“·å–å…§å®¹")
    parser.add_argument("--config", "-c", default=None,
                        help="è¨­å®šæª”è·¯å¾‘ï¼ˆé è¨­ï¼šåŒç›®éŒ„ä¸‹çš„ config.jsonï¼‰")
    parser.add_argument("--ptt-board", help="PTT çœ‹æ¿åç¨±ï¼ˆè‡ªå‹•çˆ¬å–æ–‡ç« åˆ—è¡¨ï¼‰")
    parser.add_argument("--pages", type=int, default=1,
                        help="PTT çœ‹æ¿é æ•¸ï¼ˆé è¨­ï¼š1ï¼‰")
    parser.add_argument("--schedule", type=int, metavar="MINUTES",
                        help="æ’ç¨‹æ¨¡å¼ï¼šæ¯éš” N åˆ†é˜è‡ªå‹•åŸ·è¡Œ")

    args = parser.parse_args()

    # é‡æ–°è¼‰å…¥è¨­å®šï¼ˆå¦‚æœ‰æŒ‡å®š --configï¼‰
    if args.config:
        _CONFIG = load_config(args.config)
        DEFAULT_OUTPUT_DIR = _CONFIG["output_dir"]
        REQUEST_TIMEOUT = _CONFIG["request_timeout"]
        MAX_RETRIES = _CONFIG["max_retries"]
        RETRY_BASE_DELAY = _CONFIG["retry_base_delay"]
        POLITENESS_DELAY = _CONFIG["politeness_delay"]
        JINA_BASE_URL = _CONFIG["jina_base_url"]
        if args.output == os.path.expanduser("~/vet-articles"):
            args.output = DEFAULT_OUTPUT_DIR

    # åˆå§‹åŒ–æ—¥èªŒï¼ˆconsole + fileï¼‰
    _setup_logging(log_dir=args.output, level=_CONFIG.get("log_level", "INFO"))

    if not args.url and not args.batch and not args.ptt_board:
        parser.print_help()
        sys.exit(1)

    # ç´”è­˜åˆ¥æ¨¡å¼
    if args.identify and args.url:
        info = identify_platform(args.url)
        print(json.dumps(info, ensure_ascii=False, indent=2))
        return

    # æ’ç¨‹æ¨¡å¼
    if args.schedule:
        run_scheduled(args)
        return

    # PTT çœ‹æ¿æ¨¡å¼
    if args.ptt_board:
        urls = fetch_ptt_board(args.ptt_board, args.pages, args.output)
        if urls:
            batch_fetch_urls(urls, args.output)
        else:
            logger.info("æ²’æœ‰æ–°æ–‡ç« éœ€è¦æ“·å–")
        return

    # æ‰¹æ¬¡æ¨¡å¼
    if args.batch:
        batch_fetch(args.batch, args.output)
        return

    # å–®ä¸€ URL æ¨¡å¼
    if args.url:
        if is_already_fetched(args.url, args.output):
            logger.info(f"æ­¤ URL å·²ä¸‹è¼‰éï¼š{args.url}")
            logger.info("å¦‚è¦é‡æ–°ä¸‹è¼‰ï¼Œè«‹åˆªé™¤è¼¸å‡ºç›®éŒ„ä¸­çš„ .fetched_urls.json å°æ‡‰è¨˜éŒ„")
            return
        article = fetch_article(args.url)
        if article:
            save_path = save_article(article, args.output)
            mark_as_fetched(args.url, args.output)
            logger.info(f"æ–‡ç« å·²å„²å­˜åˆ°ï¼š{save_path}")
        else:
            logger.error(f"æ“·å–å¤±æ•—ï¼š{args.url}")
            sys.exit(1)


if __name__ == "__main__":
    main()
