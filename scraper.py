#!/usr/bin/env python3
"""
ç¸é†«æ–‡ç« è‡ªå‹•åŒ–æ“·å–å·¥å…·
===========================
ä¸‰å±¤ç­–ç•¥ï¼šJina Readerï¼ˆé¦–é¸ï¼‰â†’ BeautifulSoupï¼ˆå‚™é¸ï¼‰â†’ Playwrightï¼ˆå…œåº•ï¼‰
æ”¯æ´å¹³å°ï¼šPTTã€Mediumã€æ–°èç¶²ç«™ã€éƒ¨è½æ ¼ã€ç¸é†«å­¸æœƒç¶²ç«™ç­‰

ç”¨æ³•ï¼š
  # å–®ä¸€ URL
  python scraper.py https://example.com/article

  # æ‰¹æ¬¡ï¼ˆå¾æª”æ¡ˆè®€å– URL åˆ—è¡¨ï¼‰
  python scraper.py --batch urls.txt

  # æŒ‡å®šè¼¸å‡ºç›®éŒ„
  python scraper.py https://example.com/article --output ./my_articles
"""

import os
import re
import sys
import json
import time
import hashlib
import argparse
import logging
import urllib.robotparser
import urllib.request
import urllib.error
from datetime import datetime
from functools import lru_cache
from pathlib import Path
from urllib.parse import urlparse, urljoin

import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md

# ============================================================
# è¨­å®š
# ============================================================

DEFAULT_OUTPUT_DIR = os.path.expanduser("~/vet-articles")
REQUEST_TIMEOUT = 30
JINA_BASE_URL = "https://r.jina.ai/"
JINA_API_KEY = os.environ.get("JINA_API_KEY", "")  # è¨­å®šå¾Œå¯æå‡è‡³ 200 æ¬¡/åˆ†é˜
IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg', '.bmp'}
MAX_RETRIES = 3
RETRY_BASE_DELAY = 2  # ç§’ï¼ŒæŒ‡æ•¸é€€é¿åŸºåº•
DEDUP_FILE = ".fetched_urls.json"  # å·²ä¸‹è¼‰ URL è¨˜éŒ„æª”

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
}

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


# ============================================================
# robots.txt æª¢æŸ¥
# ============================================================

@lru_cache(maxsize=128)
def _get_robots_parser(domain: str):
    """å–å¾—ä¸¦å¿«å–æŒ‡å®š domain çš„ robots.txt è§£æå™¨"""
    robots_url = f"{domain}/robots.txt"
    parser = urllib.robotparser.RobotFileParser()
    parser.set_url(robots_url)
    try:
        resp = urllib.request.urlopen(robots_url, timeout=5)
        content = resp.read().decode("utf-8", errors="surrogateescape")
        parser.parse(content.splitlines())
        return parser
    except Exception:
        return None


def is_allowed_by_robots(url: str, user_agent: str = "*") -> bool:
    """æª¢æŸ¥ robots.txt æ˜¯å¦å…è¨±æ“·å–æ­¤ URLï¼ˆfail-openï¼šç„¡æ³•å–å¾—æ™‚å…è¨±ï¼‰"""
    try:
        parsed = urlparse(url)
        domain = f"{parsed.scheme}://{parsed.netloc}"
        parser = _get_robots_parser(domain)
        if parser is None:
            return True
        return parser.can_fetch(user_agent, url)
    except Exception:
        return True


# ============================================================
# é‡è©¦æ©Ÿåˆ¶ï¼ˆæŒ‡æ•¸é€€é¿ï¼‰
# ============================================================

def retry_fetch(func, url: str, max_retries: int = MAX_RETRIES) -> dict | None:
    """å¸¶æŒ‡æ•¸é€€é¿çš„é‡è©¦åŒ…è£å™¨"""
    for attempt in range(max_retries):
        result = func(url)
        if result is not None:
            return result
        if attempt < max_retries - 1:
            delay = RETRY_BASE_DELAY * (2 ** attempt)
            logger.info(f"  ç¬¬ {attempt + 1} æ¬¡å¤±æ•—ï¼Œ{delay} ç§’å¾Œé‡è©¦...")
            time.sleep(delay)
    return None


# ============================================================
# å»é‡æ©Ÿåˆ¶
# ============================================================

def _load_dedup_record(output_dir: str) -> set:
    """è¼‰å…¥å·²ä¸‹è¼‰çš„ URL è¨˜éŒ„"""
    dedup_path = Path(output_dir) / DEDUP_FILE
    if dedup_path.exists():
        try:
            data = json.loads(dedup_path.read_text(encoding='utf-8'))
            return set(data)
        except (json.JSONDecodeError, TypeError):
            return set()
    return set()


def _save_dedup_record(output_dir: str, fetched_urls: set):
    """å„²å­˜å·²ä¸‹è¼‰çš„ URL è¨˜éŒ„"""
    dedup_path = Path(output_dir) / DEDUP_FILE
    dedup_path.parent.mkdir(parents=True, exist_ok=True)
    dedup_path.write_text(
        json.dumps(sorted(fetched_urls), ensure_ascii=False, indent=2),
        encoding='utf-8'
    )


def is_already_fetched(url: str, output_dir: str) -> bool:
    """æª¢æŸ¥ URL æ˜¯å¦å·²ç¶“ä¸‹è¼‰é"""
    fetched = _load_dedup_record(output_dir)
    return url in fetched


def mark_as_fetched(url: str, output_dir: str):
    """å°‡ URL æ¨™è¨˜ç‚ºå·²ä¸‹è¼‰"""
    fetched = _load_dedup_record(output_dir)
    fetched.add(url)
    _save_dedup_record(output_dir, fetched)


# ============================================================
# YAML å®‰å…¨è½‰ç¾©
# ============================================================

def _yaml_safe_title(title: str) -> str:
    """ç¢ºä¿æ¨™é¡Œå¯å®‰å…¨æ”¾å…¥ YAML frontmatter"""
    title = title.replace('\\', '\\\\').replace('"', '\\"')
    return title


# ============================================================
# ç¬¬ä¸€æ­¥ï¼šå¹³å°è­˜åˆ¥
# ============================================================

PLATFORM_RULES = [
    # (åç¨±, åŸŸåé—œéµå­—åˆ—è¡¨, æ˜¯å¦éœ€è¦ç™»å…¥, å»ºè­°ç­–ç•¥)
    ("PTT",        ["ptt.cc"],                          False, "jina"),
    ("Medium",     ["medium.com"],                      False, "jina"),
    ("ç—å®¢é‚¦",     ["pixnet.net"],                       False, "jina"),
    ("æ–¹æ ¼å­",     ["vocus.cc"],                         False, "jina"),
    ("LINE TODAY", ["today.line.me"],                    False, "jina"),
    ("ç¸é†«å­¸æœƒ",   ["vetmed.org.tw", "tava.org.tw",
                    "avat.org.tw"],                      False, "bs4"),
    ("æ–°èç¶²ç«™",   ["udn.com", "ltn.com.tw",
                    "ettoday.net", "setn.com",
                    "chinatimes.com", "tvbs.com.tw",
                    "cna.com.tw"],                       False, "jina"),
    ("éƒ¨è½æ ¼",     ["blogspot.com", "wordpress.com",
                    "blog."],                            False, "jina"),
    # é›£çˆ¬å¹³å°ï¼ˆæé†’ç”¨æˆ¶ä½¿ç”¨ Chrome Extensionï¼‰
    ("Facebook",   ["facebook.com", "fb.com",
                    "fb.watch"],                         True,  "skip"),
    ("Instagram",  ["instagram.com"],                   True,  "skip"),
    ("å¾®ä¿¡å…¬çœ¾è™Ÿ", ["mp.weixin.qq.com"],                 True,  "playwright"),
    ("å°ç´…æ›¸",     ["xiaohongshu.com", "xhslink.com"],  True,  "playwright"),
]


def identify_platform(url: str) -> dict:
    """è­˜åˆ¥ URL æ‰€å±¬å¹³å°ï¼Œå›å‚³å¹³å°è³‡è¨Š"""
    parsed = urlparse(url)
    domain = parsed.netloc.lower()

    for name, keywords, needs_login, strategy in PLATFORM_RULES:
        for kw in keywords:
            if kw in domain:
                return {
                    "name": name,
                    "domain": domain,
                    "needs_login": needs_login,
                    "strategy": strategy,
                }

    # æœªçŸ¥å¹³å°ï¼Œé è¨­ç”¨ Jina
    return {
        "name": "å…¶ä»–",
        "domain": domain,
        "needs_login": False,
        "strategy": "jina",
    }


# ============================================================
# ç¬¬äºŒæ­¥ï¼šJina Reader ç­–ç•¥ï¼ˆé¦–é¸ï¼Œå…è²»ï¼‰
# ============================================================

def fetch_with_jina(url: str) -> dict | None:
    """ç”¨ Jina Reader æ“·å–ç¶²é ï¼Œå›å‚³ Markdown å…§å®¹"""
    jina_url = f"{JINA_BASE_URL}{url}"
    headers = {
        **HEADERS,
        'Accept': 'text/markdown',
    }
    if JINA_API_KEY:
        headers['Authorization'] = f'Bearer {JINA_API_KEY}'

    try:
        logger.info(f"[Jina] æ­£åœ¨æ“·å–ï¼š{url}")
        resp = requests.get(jina_url, headers=headers, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()

        content = resp.text.strip()

        # é©—è­‰å…§å®¹æœ‰æ•ˆæ€§
        if len(content) < 100:
            logger.warning("[Jina] å…§å®¹å¤ªçŸ­ï¼Œå¯èƒ½æ“·å–å¤±æ•—")
            return None

        # å˜—è©¦å¾å…§å®¹æå–æ¨™é¡Œï¼ˆJina é€šå¸¸ç¬¬ä¸€è¡Œæ˜¯ Title:ï¼‰
        title = _extract_title_from_jina(content)

        return {
            "title": title,
            "content": content,
            "source": "jina",
            "url": url,
        }

    except requests.exceptions.RequestException as e:
        logger.warning(f"[Jina] æ“·å–å¤±æ•—ï¼š{e}")
        return None


def _extract_title_from_jina(content: str) -> str:
    """å¾ Jina å›å‚³çš„å…§å®¹æå–æ¨™é¡Œ"""
    lines = content.split('\n')
    for line in lines[:5]:
        line = line.strip()
        # Jina æ ¼å¼ï¼šTitle: xxxxx
        if line.startswith('Title:'):
            return line[6:].strip()
        # Markdown H1
        if line.startswith('# '):
            return line[2:].strip()
    return "æœªå‘½åæ–‡ç« "


# ============================================================
# ç¬¬ä¸‰æ­¥ï¼šBeautifulSoup ç­–ç•¥ï¼ˆå‚™é¸ï¼‰
# ============================================================

def fetch_with_bs4(url: str) -> dict | None:
    """ç”¨ requests + BeautifulSoup æ“·å–ç¶²é """
    try:
        logger.info(f"[BS4] æ­£åœ¨æ“·å–ï¼š{url}")
        resp = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()
        resp.encoding = resp.apparent_encoding or 'utf-8'  # è‡ªå‹•åµæ¸¬ç·¨ç¢¼ï¼ŒNone æ™‚å›é€€ UTF-8

        soup = BeautifulSoup(resp.text, 'html.parser')

        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for tag in soup.find_all(['script', 'style', 'nav', 'footer',
                                   'header', 'aside', 'iframe', 'noscript']):
            tag.decompose()

        # å˜—è©¦æ‰¾åˆ°ä¸»è¦å…§å®¹å€åŸŸ
        article = (
            soup.find('article') or
            soup.find('div', class_=re.compile(r'article|content|post|entry', re.I)) or
            soup.find('div', id=re.compile(r'article|content|post|entry', re.I)) or
            soup.find('main') or
            soup.body
        )

        if not article:
            logger.warning("[BS4] æ‰¾ä¸åˆ°ä¸»è¦å…§å®¹")
            return None

        # è½‰æ›æˆ Markdown
        content = md(str(article), heading_style="ATX", strip=['img'])

        # æå–åœ–ç‰‡
        images = []
        for img in article.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-original')
            if src:
                full_url = urljoin(url, src)
                images.append(full_url)

        # é‡å»ºå«åœ–ç‰‡çš„ Markdown
        for i, img_url in enumerate(images):
            content += f"\n\n![åœ–ç‰‡{i+1}]({img_url})"

        # æå–æ¨™é¡Œ
        title = ""
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text(strip=True)
        h1 = soup.find('h1')
        if h1:
            title = h1.get_text(strip=True)

        if len(content.strip()) < 50:
            logger.warning("[BS4] å…§å®¹å¤ªçŸ­")
            return None

        return {
            "title": title or "æœªå‘½åæ–‡ç« ",
            "content": content.strip(),
            "source": "bs4",
            "url": url,
        }

    except requests.exceptions.RequestException as e:
        logger.warning(f"[BS4] æ“·å–å¤±æ•—ï¼š{e}")
        return None


# ============================================================
# ç¬¬å››æ­¥ï¼šè‡ªå‹•é™ç´šæ“·å–
# ============================================================

def fetch_article(url: str) -> dict | None:
    """
    è‡ªå‹•è­˜åˆ¥å¹³å°ä¸¦ç”¨æœ€ä½³ç­–ç•¥æ“·å–æ–‡ç« ã€‚
    å« robots.txt æª¢æŸ¥ã€é‡è©¦æ©Ÿåˆ¶ã€é™ç´šé †åºï¼šJina â†’ BS4
    """
    platform = identify_platform(url)
    logger.info(f"å¹³å°è­˜åˆ¥ï¼š{platform['name']} ({platform['domain']})")

    # éœ€è¦ç™»å…¥çš„å¹³å°ï¼Œæé†’ç”¨æˆ¶
    if platform["strategy"] == "skip":
        logger.warning(
            f"âš ï¸  {platform['name']} éœ€è¦ç™»å…¥æ‰èƒ½æ“·å–ï¼Œ"
            f"å»ºè­°ä½¿ç”¨ Chrome Extension æ‰‹å‹•å„²å­˜ï¼"
        )
        return None

    # robots.txt æª¢æŸ¥
    if not is_allowed_by_robots(url):
        logger.warning(f"ğŸš« robots.txt ä¸å…è¨±æ“·å–ï¼š{url}")
        return None

    # æ ¹æ“šå»ºè­°ç­–ç•¥æ±ºå®šå˜—è©¦é †åº
    if platform["strategy"] == "bs4":
        strategies = [("bs4", fetch_with_bs4), ("jina", fetch_with_jina)]
    else:
        strategies = [("jina", fetch_with_jina), ("bs4", fetch_with_bs4)]

    for name, func in strategies:
        result = retry_fetch(func, url)
        if result:
            logger.info(f"âœ… æˆåŠŸæ“·å–ï¼ˆç­–ç•¥ï¼š{name}ï¼‰")
            result["platform"] = platform["name"]
            return result
        logger.info(f"[{name}] æ‰€æœ‰é‡è©¦å‡å¤±æ•—ï¼Œå˜—è©¦ä¸‹ä¸€å€‹ç­–ç•¥...")

    logger.error(f"âŒ æ‰€æœ‰ç­–ç•¥éƒ½å¤±æ•—ï¼š{url}")
    return None


# ============================================================
# ç¬¬äº”æ­¥ï¼šåœ–ç‰‡ä¸‹è¼‰èˆ‡å…§å®¹ä¿å­˜
# ============================================================

def download_image(img_url: str, save_path: Path, referer: str = "") -> bool:
    """ä¸‹è¼‰å–®å¼µåœ–ç‰‡"""
    try:
        headers = {**HEADERS}
        if referer:
            headers['Referer'] = referer

        resp = requests.get(img_url, headers=headers, timeout=15, stream=True)
        resp.raise_for_status()

        with open(save_path, 'wb') as f:
            for chunk in resp.iter_content(8192):
                f.write(chunk)
        return True

    except Exception as e:
        logger.warning(f"åœ–ç‰‡ä¸‹è¼‰å¤±æ•—ï¼š{img_url} â€” {e}")
        return False


def save_article(article: dict, output_dir: str = DEFAULT_OUTPUT_DIR) -> Path:
    """
    å„²å­˜æ–‡ç« ç‚º Markdownï¼Œä¸‹è¼‰åœ–ç‰‡åˆ°æœ¬åœ°ã€‚
    ç›®éŒ„çµæ§‹ï¼šoutput_dir/YYYY-MM-DD_æ¨™é¡Œ/content.md
    """
    title = article["title"]
    # æ¸…ç†æ¨™é¡Œä¸­çš„ç‰¹æ®Šå­—å…ƒ
    safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)[:60]
    date_str = datetime.now().strftime("%Y-%m-%d")
    folder_name = f"{date_str}_{safe_title}"

    # é˜²æ­¢æ¨™é¡Œç¢°æ’ï¼šè³‡æ–™å¤¾å·²å­˜åœ¨æ™‚åŠ ä¸ŠçŸ­ hash
    article_dir = Path(output_dir) / folder_name
    if article_dir.exists():
        url_hash = hashlib.md5(article.get("url", "").encode()).hexdigest()[:6]
        folder_name = f"{folder_name}_{url_hash}"
        article_dir = Path(output_dir) / folder_name
    images_dir = article_dir / "images"
    article_dir.mkdir(parents=True, exist_ok=True)
    images_dir.mkdir(exist_ok=True)

    content = article["content"]

    # æå–ä¸¦ä¸‹è¼‰åœ–ç‰‡
    img_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
    img_matches = re.findall(img_pattern, content)

    # ä¹Ÿæ‰¾ç´” URL åœ–ç‰‡
    url_pattern = r'(https?://[^\s<>"\']+\.(?:jpg|jpeg|png|gif|webp)(?:\?[^\s<>"\']*)?)'
    url_matches = re.findall(url_pattern, content)

    all_images = []
    for alt, url in img_matches:
        all_images.append(url)
    for url in url_matches:
        if url not in all_images:
            all_images.append(url)

    # æ±ºå®š Referer
    referer = article.get("url", "")
    parsed = urlparse(referer)
    referer_base = f"{parsed.scheme}://{parsed.netloc}/"

    # ä¸‹è¼‰åœ–ç‰‡ä¸¦æ›¿æ›è·¯å¾‘
    for i, img_url in enumerate(all_images, 1):
        ext = _guess_extension(img_url)
        local_name = f"img_{i:02d}{ext}"
        local_path = images_dir / local_name

        if download_image(img_url, local_path, referer=referer_base):
            # æ›¿æ›å…§å®¹ä¸­çš„åœ–ç‰‡è·¯å¾‘
            content = content.replace(img_url, f"images/{local_name}")
            logger.info(f"  ğŸ“· åœ–ç‰‡ {i}: {local_name}")

    # çµ„è£æœ€çµ‚ Markdownï¼ˆYAML å®‰å…¨è½‰ç¾©æ¨™é¡Œï¼‰
    yaml_title = _yaml_safe_title(title)
    metadata = f"""---
title: "{yaml_title}"
source: {article.get('url', 'N/A')}
platform: {article.get('platform', 'unknown')}
fetched_by: {article.get('source', 'unknown')}
date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
---

"""
    final_content = metadata + content

    # å„²å­˜
    md_path = article_dir / "content.md"
    md_path.write_text(final_content, encoding='utf-8')

    # åŒæ™‚å„²å­˜åŸå§‹ JSONï¼ˆæ–¹ä¾¿å¾ŒçºŒæ‰¹æ¬¡è™•ç†ï¼‰
    meta_path = article_dir / "metadata.json"
    meta_path.write_text(json.dumps({
        "title": title,
        "url": article.get("url"),
        "platform": article.get("platform"),
        "source": article.get("source"),
        "fetched_at": datetime.now().isoformat(),
        "image_count": len(all_images),
    }, ensure_ascii=False, indent=2), encoding='utf-8')

    logger.info(f"ğŸ’¾ å·²å„²å­˜ï¼š{md_path}")
    return article_dir


def _guess_extension(url: str) -> str:
    """å¾ URL çŒœæ¸¬åœ–ç‰‡å‰¯æª”å"""
    parsed = urlparse(url)
    path = parsed.path.lower()
    for ext in IMAGE_EXTENSIONS:
        if path.endswith(ext):
            return ext
    return '.jpg'  # é è¨­


# ============================================================
# ç¬¬å…­æ­¥ï¼šæ‰¹æ¬¡è™•ç†
# ============================================================

def batch_fetch(url_file: str, output_dir: str = DEFAULT_OUTPUT_DIR) -> dict:
    """
    å¾æª”æ¡ˆè®€å– URL åˆ—è¡¨ï¼Œæ‰¹æ¬¡æ“·å–ã€‚
    URL æª”æ¡ˆæ ¼å¼ï¼šæ¯è¡Œä¸€å€‹ URLï¼Œ# é–‹é ­ç‚ºè¨»è§£
    """
    urls = []
    with open(url_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                urls.append(line)

    logger.info(f"ğŸ“‹ å…± {len(urls)} å€‹ URL å¾…æ“·å–")

    results = {"success": [], "failed": [], "skipped": []}

    for i, url in enumerate(urls, 1):
        logger.info(f"\n--- [{i}/{len(urls)}] ---")

        # å»é‡æª¢æŸ¥
        if is_already_fetched(url, output_dir):
            logger.info(f"â­ï¸  å·²ä¸‹è¼‰éï¼Œè·³éï¼š{url}")
            results["skipped"].append({"url": url, "reason": "å·²ä¸‹è¼‰é"})
            continue

        platform = identify_platform(url)
        if platform["strategy"] == "skip":
            logger.warning(f"â­ï¸  è·³é {platform['name']} å¹³å°ï¼š{url}")
            results["skipped"].append({"url": url, "reason": f"{platform['name']} éœ€è¦ç™»å…¥"})
            continue

        article = fetch_article(url)
        if article:
            save_path = save_article(article, output_dir)
            mark_as_fetched(url, output_dir)
            results["success"].append({"url": url, "path": str(save_path)})
        else:
            results["failed"].append({"url": url})

        # ç¦®è²Œå»¶é²ï¼Œé¿å…è¢«å°
        if i < len(urls):
            time.sleep(2)

    # è¼¸å‡ºçµ±è¨ˆ
    logger.info(f"\n{'='*50}")
    logger.info(f"ğŸ“Š æ“·å–å®Œæˆï¼")
    logger.info(f"   âœ… æˆåŠŸï¼š{len(results['success'])}")
    logger.info(f"   âŒ å¤±æ•—ï¼š{len(results['failed'])}")
    logger.info(f"   â­ï¸  è·³éï¼š{len(results['skipped'])}")

    # å„²å­˜å ±å‘Š
    report_path = Path(output_dir) / f"batch_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    report_path.parent.mkdir(parents=True, exist_ok=True)
    report_path.write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding='utf-8')
    logger.info(f"   ğŸ“„ å ±å‘Šï¼š{report_path}")

    return results


# ============================================================
# CLI å…¥å£
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="ğŸ¾ ç¸é†«æ–‡ç« è‡ªå‹•åŒ–æ“·å–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ï¼š
  # æ“·å–å–®ç¯‡æ–‡ç« 
  python scraper.py https://www.ptt.cc/bbs/dog/M.xxxxx.html

  # æ‰¹æ¬¡æ“·å–ï¼ˆå¾æª”æ¡ˆè®€å– URLï¼‰
  python scraper.py --batch urls.txt

  # æŒ‡å®šè¼¸å‡ºç›®éŒ„
  python scraper.py https://example.com --output ~/obsidian/vet-articles

  # åªè­˜åˆ¥å¹³å°ï¼ˆä¸æ“·å–ï¼‰
  python scraper.py https://facebook.com/some-post --identify
        """
    )
    parser.add_argument("url", nargs="?", help="è¦æ“·å–çš„ç¶²é  URL")
    parser.add_argument("--batch", "-b", help="æ‰¹æ¬¡æ¨¡å¼ï¼šURL åˆ—è¡¨æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--output", "-o", default=DEFAULT_OUTPUT_DIR,
                        help=f"è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­ï¼š{DEFAULT_OUTPUT_DIR}ï¼‰")
    parser.add_argument("--identify", "-i", action="store_true",
                        help="åªè­˜åˆ¥å¹³å°ï¼Œä¸æ“·å–å…§å®¹")

    args = parser.parse_args()

    if not args.url and not args.batch:
        parser.print_help()
        sys.exit(1)

    # ç´”è­˜åˆ¥æ¨¡å¼
    if args.identify and args.url:
        info = identify_platform(args.url)
        print(json.dumps(info, ensure_ascii=False, indent=2))
        return

    # æ‰¹æ¬¡æ¨¡å¼
    if args.batch:
        batch_fetch(args.batch, args.output)
        return

    # å–®ä¸€ URL æ¨¡å¼
    if args.url:
        if is_already_fetched(args.url, args.output):
            logger.info(f"æ­¤ URL å·²ä¸‹è¼‰éï¼š{args.url}")
            logger.info("å¦‚è¦é‡æ–°ä¸‹è¼‰ï¼Œè«‹åˆªé™¤è¼¸å‡ºç›®éŒ„ä¸­çš„ .fetched_urls.json å°æ‡‰è¨˜éŒ„")
            return
        article = fetch_article(args.url)
        if article:
            save_path = save_article(article, args.output)
            mark_as_fetched(args.url, args.output)
            logger.info(f"æ–‡ç« å·²å„²å­˜åˆ°ï¼š{save_path}")
        else:
            logger.error(f"æ“·å–å¤±æ•—ï¼š{args.url}")
            sys.exit(1)


if __name__ == "__main__":
    main()
